"""
æ‰¹é‡ç”Ÿå­˜åˆ†æè„šæœ¬
================

åŠŸèƒ½æ¦‚è¿°ï¼š
    å¯¹æ‰€æœ‰åŸºå› è¿›è¡Œæ‰¹é‡ç”Ÿå­˜åˆ†æï¼Œè¯†åˆ«ä¸æ‚£è€…ç”Ÿå­˜é¢„åæœ€ç›¸å…³çš„åŸºå› ã€‚
    ä½¿ç”¨Log-rankæ£€éªŒè¯„ä¼°æ¯ä¸ªåŸºå› çš„ç”Ÿå­˜åˆ†ææ˜¾è‘—æ€§ï¼Œ
    å¹¶ä¸ºæœ€æ˜¾è‘—çš„åŸºå› ç”ŸæˆKaplan-Meierç”Ÿå­˜æ›²çº¿å›¾ã€‚

ä¸»è¦åŠŸèƒ½ï¼š
    1. æ‰¹é‡å¤„ç†æ‰€æœ‰åŸºå› çš„ç”Ÿå­˜åˆ†æ
    2. ä½¿ç”¨Log-rankæ£€éªŒè¯„ä¼°ç»Ÿè®¡æ˜¾è‘—æ€§
    3. ç­›é€‰æœ€æ˜¾è‘—çš„åŸºå› ï¼ˆTop Nï¼‰
    4. ç”Ÿæˆè¯¦ç»†çš„ç”Ÿå­˜åˆ†ææŠ¥å‘Š
    5. ä¸ºé‡è¦åŸºå› åˆ›å»ºé«˜è´¨é‡çš„KMæ›²çº¿å›¾

è¾“å…¥ï¼š
    - åŸºå› è¡¨è¾¾æ•°æ®æ–‡ä»¶ï¼ˆCSVæ ¼å¼ï¼‰
    - ä¸´åºŠæ•°æ®æ–‡ä»¶ï¼ˆåˆ¶è¡¨ç¬¦åˆ†éš”ï¼‰
    
è¾“å‡ºï¼š
    - æ‰€æœ‰åŸºå› çš„ç”Ÿå­˜åˆ†æç»“æœè¡¨æ ¼
    - æœ€æ˜¾è‘—åŸºå› çš„KMæ›²çº¿å›¾
    - è¯¦ç»†çš„åˆ†ææ—¥å¿—å’Œç»Ÿè®¡æŠ¥å‘Š

ä½œè€…ï¼šBRM_Mitacsé¡¹ç›®ç»„
ç‰ˆæœ¬ï¼š1.0
"""

import argparse
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
from lifelines import KaplanMeierFitter
from lifelines.statistics import logrank_test
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime
import logging
import sys
from tqdm import tqdm
import multiprocessing as mp
from functools import partial
import re

# å¿½ç•¥è­¦å‘Šä¿¡æ¯
warnings.filterwarnings('ignore')

# å¯¼å…¥Coxå›å½’ç›¸å…³åº“
from lifelines import CoxPHFitter

# å®šä¹‰é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).resolve().parents[1]

def sanitize_filename(filename):
    """
    æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦
    
    å‚æ•°ï¼š
        filename (str): åŸå§‹æ–‡ä»¶å
        
    è¿”å›ï¼š
        str: æ¸…ç†åçš„å®‰å…¨æ–‡ä»¶å
    """
    # Windowsæ–‡ä»¶åéæ³•å­—ç¬¦: < > : " | ? * / \
    illegal_chars = r'[<>:"|?*/\\]'
    # å°†éæ³•å­—ç¬¦æ›¿æ¢ä¸ºä¸‹åˆ’çº¿
    sanitized = re.sub(illegal_chars, '_', filename)
    # ç§»é™¤è¿ç»­çš„ä¸‹åˆ’çº¿
    sanitized = re.sub(r'_+', '_', sanitized)
    # ç§»é™¤é¦–å°¾çš„ä¸‹åˆ’çº¿å’Œç‚¹å·
    sanitized = sanitized.strip('_.')
    # ç¡®ä¿æ–‡ä»¶åä¸ä¸ºç©º
    if not sanitized:
        sanitized = 'unnamed'
    return sanitized

def setup_logging(output_dir):
    """
    è®¾ç½®æ—¥å¿—è®°å½•ç³»ç»Ÿ
    
    å‚æ•°ï¼š
        output_dir (Path): æ—¥å¿—æ–‡ä»¶ä¿å­˜ç›®å½•
    """
    log_file = output_dir / "batch_survival_analysis_log.txt"
    
    logger = logging.getLogger('BatchSurvivalAnalysis')
    logger.setLevel(logging.INFO)
    
    # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
    for handler in logger.handlers[:]:
        logger.removeHandler(handler)
    
    # æ–‡ä»¶å¤„ç†å™¨
    file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
    file_handler.setLevel(logging.INFO)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(logging.INFO)
    
    # æ ¼å¼åŒ–å™¨
    formatter = logging.Formatter(
        '%(asctime)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    file_handler.setFormatter(formatter)
    console_handler.setFormatter(formatter)
    
    logger.addHandler(file_handler)
    logger.addHandler(console_handler)
    
    return logger

def prepare_survival_data(expression_file, clinical_file, logger):
    """
    å‡†å¤‡ç”Ÿå­˜åˆ†ææ•°æ®
    
    å‚æ•°ï¼š
        expression_file (Path): åŸºå› è¡¨è¾¾æ•°æ®æ–‡ä»¶è·¯å¾„
        clinical_file (Path): ä¸´åºŠæ•°æ®æ–‡ä»¶è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
        
    è¿”å›ï¼š
        tuple: (åŸºå› è¡¨è¾¾æ•°æ®DataFrame, åˆå¹¶çš„ç”Ÿå­˜æ•°æ®DataFrame)
    """
    logger.info("="*80)
    logger.info("å¼€å§‹æ•°æ®åŠ è½½å’Œé¢„å¤„ç†")
    logger.info("="*80)
    
    # 1. åŠ è½½æ•°æ®
    logger.info("æ­£åœ¨åŠ è½½åŸºå› è¡¨è¾¾å’Œä¸´åºŠæ•°æ®...")
    try:
        expr_df = pd.read_csv(expression_file, index_col=0)
        clin_df = pd.read_csv(clinical_file, sep='\t', index_col=0).T
        logger.info(f"âœ“ åŸå§‹è¡¨è¾¾æ•°æ®ç»´åº¦: {expr_df.shape}")
        logger.info(f"âœ“ åŸå§‹ä¸´åºŠæ•°æ®ç»´åº¦: {clin_df.shape}")
    except FileNotFoundError as e:
        logger.error(f"âœ— æ•°æ®åŠ è½½é”™è¯¯: {e}")
        return None, None
    
    # 2. æ ‡å‡†åŒ–æ ·æœ¬ID
    logger.info("æ­£åœ¨æ ‡å‡†åŒ–æ ·æœ¬ID...")
    expr_df.columns = expr_df.columns.str.upper().str.replace('.', '-').str.slice(0, 12)
    clin_df.index = clin_df.index.str.upper().str.slice(0, 12)
    
    # æ£€æŸ¥å¹¶æ˜¾ç¤ºé‡å¤æ ·æœ¬
    original_expr_samples = len(expr_df.columns)
    original_clin_samples = len(clin_df)
    
    # æ‰¾å‡ºè¡¨è¾¾æ•°æ®ä¸­çš„é‡å¤æ ·æœ¬
    expr_duplicated = expr_df.columns.duplicated(keep=False)  # æ ‡è®°æ‰€æœ‰é‡å¤é¡¹
    if expr_duplicated.any():
        duplicated_expr_samples = expr_df.columns[expr_duplicated].unique()
        logger.info(f"å‘ç°è¡¨è¾¾æ•°æ®ä¸­çš„é‡å¤æ ·æœ¬ ({len(duplicated_expr_samples)} ä¸ª):")
        for sample in duplicated_expr_samples:
            count = (expr_df.columns == sample).sum()
            logger.info(f"  - {sample}: å‡ºç° {count} æ¬¡")
    else:
        logger.info("è¡¨è¾¾æ•°æ®ä¸­æœªå‘ç°é‡å¤æ ·æœ¬")
    
    # æ‰¾å‡ºä¸´åºŠæ•°æ®ä¸­çš„é‡å¤æ ·æœ¬
    clin_duplicated = clin_df.index.duplicated(keep=False)  # æ ‡è®°æ‰€æœ‰é‡å¤é¡¹
    if clin_duplicated.any():
        duplicated_clin_samples = clin_df.index[clin_duplicated].unique()
        logger.info(f"å‘ç°ä¸´åºŠæ•°æ®ä¸­çš„é‡å¤æ ·æœ¬ ({len(duplicated_clin_samples)} ä¸ª):")
        for sample in duplicated_clin_samples:
            count = (clin_df.index == sample).sum()
            logger.info(f"  - {sample}: å‡ºç° {count} æ¬¡")
    else:
        logger.info("ä¸´åºŠæ•°æ®ä¸­æœªå‘ç°é‡å¤æ ·æœ¬")
    
    # å»é‡ï¼ˆä¿ç•™ç¬¬ä¸€ä¸ªå‡ºç°çš„æ ·æœ¬ï¼‰
    expr_df = expr_df.loc[:, ~expr_df.columns.duplicated(keep='first')]
    clin_df = clin_df[~clin_df.index.duplicated(keep='first')]
    
    logger.info(f"è¡¨è¾¾æ•°æ®å»é‡å: {len(expr_df.columns)} / {original_expr_samples} ä¸ªå”¯ä¸€æ ·æœ¬")
    logger.info(f"ä¸´åºŠæ•°æ®å»é‡å: {len(clin_df)} / {original_clin_samples} ä¸ªå”¯ä¸€æ ·æœ¬")
    
    # 3. æ‰¾åˆ°å…±åŒæ ·æœ¬
    common_samples = list(set(expr_df.columns) & set(clin_df.index))
    logger.info(f"æ‰¾åˆ° {len(common_samples)} ä¸ªå…±åŒæ ·æœ¬")
    
    if len(common_samples) < 50:
        logger.error(f"âœ— å…±åŒæ ·æœ¬æ•°é‡å¤ªå°‘ ({len(common_samples)})ï¼Œæ— æ³•è¿›è¡Œå¯é çš„ç”Ÿå­˜åˆ†æ")
        return None, None
    
    # 4. å‡†å¤‡ç”Ÿå­˜æ•°æ®
    logger.info("æ­£åœ¨å¤„ç†ç”Ÿå­˜æ•°æ®...")
    survival_df = clin_df.loc[common_samples].copy()
    
    # åˆ›å»ºç”Ÿå­˜äº‹ä»¶å’Œæ—¶é—´åˆ—
    survival_df['event'] = pd.to_numeric(survival_df['vital_status'], errors='coerce')
    days_to_death = pd.to_numeric(survival_df['days_to_death'], errors='coerce')
    days_to_last_followup = pd.to_numeric(survival_df['days_to_last_followup'], errors='coerce')
    survival_df['time'] = np.where(survival_df['event'] == 1, days_to_death, days_to_last_followup)
    
    # æ¸…ç†ç”Ÿå­˜æ•°æ®
    original_survival_samples = len(survival_df)
    survival_df = survival_df.dropna(subset=['time', 'event'])
    survival_df = survival_df[survival_df['time'] > 0]  # ç§»é™¤æ—¶é—´ä¸º0æˆ–è´Ÿæ•°çš„æ ·æœ¬
    
    logger.info(f"ç”Ÿå­˜æ•°æ®æ¸…ç†: {len(survival_df)} / {original_survival_samples} ä¸ªæ ·æœ¬æœ‰å®Œæ•´ç”Ÿå­˜æ•°æ®")
    
    # 5. ç­›é€‰è¡¨è¾¾æ•°æ®
    valid_samples = survival_df.index.tolist()
    expr_filtered = expr_df[valid_samples]
    
    # 6. å¤„ç†å¹´é¾„ä¿¡æ¯
    logger.info("æ­£åœ¨å¤„ç†å¹´é¾„ä¿¡æ¯...")
    
    # æ£€æŸ¥å¹´é¾„æ•°æ®
    if 'years_to_birth' in survival_df.columns:
        survival_df['age'] = pd.to_numeric(survival_df['years_to_birth'], errors='coerce')
        
        # æ£€æŸ¥å¹´é¾„æ•°æ®å®Œæ•´æ€§
        age_missing = survival_df['age'].isna().sum()
        age_valid = len(survival_df) - age_missing
        
        logger.info(f"å¹´é¾„æ•°æ®ç»Ÿè®¡:")
        logger.info(f"  - æœ‰æ•ˆå¹´é¾„æ•°æ®: {age_valid} / {len(survival_df)} ä¸ªæ ·æœ¬")
        logger.info(f"  - ç¼ºå¤±å¹´é¾„æ•°æ®: {age_missing} ä¸ªæ ·æœ¬")
        logger.info(f"  - å¹´é¾„èŒƒå›´: [{survival_df['age'].min():.0f}, {survival_df['age'].max():.0f}] å²")
        logger.info(f"  - å¹³å‡å¹´é¾„: {survival_df['age'].mean():.1f} Â± {survival_df['age'].std():.1f} å²")
        logger.info(f"  - ä¸­ä½å¹´é¾„: {survival_df['age'].median():.0f} å²")
        
        # å¹´é¾„åˆ†ç»„åˆ†æ
        young_threshold = survival_df['age'].quantile(0.33)  # å¹´è½»ç»„ï¼ˆå‰1/3ï¼‰
        old_threshold = survival_df['age'].quantile(0.67)    # å¹´è€ç»„ï¼ˆå1/3ï¼‰
        
        survival_df['age_group'] = 'middle'
        survival_df.loc[survival_df['age'] <= young_threshold, 'age_group'] = 'young'
        survival_df.loc[survival_df['age'] >= old_threshold, 'age_group'] = 'old'
        
        age_group_counts = survival_df['age_group'].value_counts()
        logger.info(f"  - å¹´é¾„åˆ†ç»„: å¹´è½»ç»„â‰¤{young_threshold:.0f}å²({age_group_counts.get('young', 0)}äºº), "
                   f"ä¸­å¹´ç»„({age_group_counts.get('middle', 0)}äºº), "
                   f"å¹´è€ç»„â‰¥{old_threshold:.0f}å²({age_group_counts.get('old', 0)}äºº)")
        
        # æ£€æŸ¥å¹´é¾„ä¸ç”Ÿå­˜çš„å…³ç³»
        if age_valid >= 50:  # åªæœ‰è¶³å¤Ÿæ ·æœ¬æ—¶æ‰è¿›è¡Œåˆ†æ
            from lifelines.statistics import logrank_test
            young_group = survival_df[survival_df['age_group'] == 'young']
            old_group = survival_df[survival_df['age_group'] == 'old']
            
            if len(young_group) >= 10 and len(old_group) >= 10:
                age_survival_test = logrank_test(
                    young_group['time'], old_group['time'],
                    young_group['event'], old_group['event']
                )
                logger.info(f"  - å¹´é¾„ä¸ç”Ÿå­˜å…³è”æ€§æ£€éªŒ: P = {age_survival_test.p_value:.3f}")
                if age_survival_test.p_value < 0.05:
                    logger.warning(f"  âš ï¸ å¹´é¾„æ˜¾è‘—å½±å“ç”Ÿå­˜é¢„å (P < 0.05)ï¼Œå»ºè®®åœ¨åˆ†æä¸­è°ƒæ•´å¹´é¾„å› ç´ ")
                else:
                    logger.info(f"  âœ“ å¹´é¾„å¯¹ç”Ÿå­˜é¢„åå½±å“ä¸æ˜¾è‘— (P â‰¥ 0.05)")
        
        # ç§»é™¤å¹´é¾„ç¼ºå¤±çš„æ ·æœ¬
        if age_missing > 0:
            logger.info(f"ç§»é™¤ {age_missing} ä¸ªå¹´é¾„æ•°æ®ç¼ºå¤±çš„æ ·æœ¬")
            survival_df = survival_df.dropna(subset=['age'])
            valid_samples = survival_df.index.tolist()
            expr_filtered = expr_df[valid_samples]
    else:
        logger.warning("ä¸´åºŠæ•°æ®ä¸­æœªæ‰¾åˆ°å¹´é¾„ä¿¡æ¯ (years_to_birth)ï¼Œå°†æ— æ³•è°ƒæ•´å¹´é¾„å› ç´ ")
        survival_df['age'] = None
    
    logger.info(f"æœ€ç»ˆç”¨äºåˆ†æçš„æ•°æ®:")
    logger.info(f"  - åŸºå› æ•°é‡: {len(expr_filtered)}")
    logger.info(f"  - æ ·æœ¬æ•°é‡: {len(expr_filtered.columns)}")
    logger.info(f"  - äº‹ä»¶æ•°é‡ (æ­»äº¡): {survival_df['event'].sum()}")
    logger.info(f"  - åˆ å¤±æ•°é‡ (å­˜æ´»): {(survival_df['event'] == 0).sum()}")
    
    # 7. æ•°æ®è´¨é‡æ£€æŸ¥å’Œé¢„è§ˆä¿å­˜
    logger.info("="*80)
    logger.info("æ•°æ®è´¨é‡æ£€æŸ¥")
    logger.info("="*80)
    
    # æ£€æŸ¥è¡¨è¾¾æ•°æ®åˆ†å¸ƒ
    logger.info("è¡¨è¾¾æ•°æ®ç»Ÿè®¡:")
    logger.info(f"  - è¡¨è¾¾å€¼èŒƒå›´: [{expr_filtered.values.min():.4f}, {expr_filtered.values.max():.4f}]")
    logger.info(f"  - è¡¨è¾¾å€¼å‡å€¼: {expr_filtered.values.mean():.4f}")
    logger.info(f"  - è¡¨è¾¾å€¼æ ‡å‡†å·®: {expr_filtered.values.std():.4f}")
    logger.info(f"  - é›¶å€¼æ¯”ä¾‹: {(expr_filtered.values == 0).sum() / expr_filtered.size * 100:.2f}%")
    
    # æ£€æŸ¥ç”Ÿå­˜æ—¶é—´åˆ†å¸ƒ
    logger.info("ç”Ÿå­˜æ—¶é—´ç»Ÿè®¡:")
    logger.info(f"  - æ—¶é—´èŒƒå›´: [{survival_df['time'].min():.0f}, {survival_df['time'].max():.0f}] å¤©")
    logger.info(f"  - ä¸­ä½éšè®¿æ—¶é—´: {survival_df['time'].median():.0f} å¤©")
    logger.info(f"  - å¹³å‡éšè®¿æ—¶é—´: {survival_df['time'].mean():.0f} å¤©")
    
    # ä¿å­˜æ•°æ®é¢„è§ˆ
    output_dir = PROJECT_ROOT / "results" / "batch_survival_analysis"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    logger.info("="*80)
    logger.info("ä¿å­˜æ•°æ®é¢„è§ˆæ–‡ä»¶")
    logger.info("="*80)
    
    # ä¿å­˜ç”Ÿå­˜æ•°æ®é¢„è§ˆ
    survival_preview = survival_df[['time', 'event', 'vital_status', 'days_to_death', 'days_to_last_followup']].copy()
    survival_preview_file = output_dir / "survival_data_preview.csv"
    survival_preview.to_csv(survival_preview_file)
    logger.info(f"âœ“ ç”Ÿå­˜æ•°æ®é¢„è§ˆå·²ä¿å­˜: {survival_preview_file.name}")
    
    # ä¿å­˜å‰10ä¸ªåŸºå› çš„è¡¨è¾¾æ•°æ®é¢„è§ˆ
    expr_preview = expr_filtered.head(10).T  # è½¬ç½®ï¼Œæ ·æœ¬ä¸ºè¡Œï¼ŒåŸºå› ä¸ºåˆ—
    expr_preview_file = output_dir / "expression_data_preview_top10_genes.csv"
    expr_preview.to_csv(expr_preview_file)
    logger.info(f"âœ“ è¡¨è¾¾æ•°æ®é¢„è§ˆ(å‰10ä¸ªåŸºå› )å·²ä¿å­˜: {expr_preview_file.name}")
    
    # ä¿å­˜æ•°æ®ç»´åº¦ä¿¡æ¯
    data_info = {
        'metric': [
            'total_genes', 'total_samples', 'death_events', 'censored_events',
            'expr_min', 'expr_max', 'expr_mean', 'expr_std', 'zero_proportion_pct',
            'time_min_days', 'time_max_days', 'time_median_days', 'time_mean_days'
        ],
        'value': [
            len(expr_filtered), len(expr_filtered.columns), survival_df['event'].sum(), 
            (survival_df['event'] == 0).sum(), expr_filtered.values.min(), expr_filtered.values.max(),
            expr_filtered.values.mean(), expr_filtered.values.std(), 
            (expr_filtered.values == 0).sum() / expr_filtered.size * 100,
            survival_df['time'].min(), survival_df['time'].max(),
            survival_df['time'].median(), survival_df['time'].mean()
        ]
    }
    data_info_df = pd.DataFrame(data_info)
    data_info_file = output_dir / "data_summary_statistics.csv"
    data_info_df.to_csv(data_info_file, index=False)
    logger.info(f"âœ“ æ•°æ®ç»Ÿè®¡ä¿¡æ¯å·²ä¿å­˜: {data_info_file.name}")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰è¡¨è¾¾æ•°æ®ä¸ºå¸¸æ•°çš„åŸºå› 
    constant_genes = []
    for gene in expr_filtered.index:
        if expr_filtered.loc[gene].std() == 0:
            constant_genes.append(gene)
    
    if constant_genes:
        logger.warning(f"å‘ç° {len(constant_genes)} ä¸ªè¡¨è¾¾å€¼ä¸ºå¸¸æ•°çš„åŸºå› ï¼Œå°†è¢«æ’é™¤")
        expr_filtered = expr_filtered.drop(constant_genes)
        logger.info(f"æ’é™¤å¸¸æ•°åŸºå› åï¼Œå‰©ä½™ {len(expr_filtered)} ä¸ªåŸºå› ")
    
    logger.info("="*80)
    logger.info("æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œå¯ä»¥å¼€å§‹åˆ†æ")
    logger.info("="*80)
    
    return expr_filtered, survival_df

def analyze_single_gene_with_age_adjustment(gene_data, survival_df, gene_name):
    """
    å¯¹å•ä¸ªåŸºå› è¿›è¡Œè°ƒæ•´å¹´é¾„çš„ç”Ÿå­˜åˆ†æ
    
    å‚æ•°ï¼š
        gene_data (Series): å•ä¸ªåŸºå› çš„è¡¨è¾¾æ•°æ®
        survival_df (DataFrame): ç”Ÿå­˜æ•°æ®ï¼ˆåŒ…å«å¹´é¾„ä¿¡æ¯ï¼‰
        gene_name (str): åŸºå› åç§°
        
    è¿”å›ï¼š
        dict: åˆ†æç»“æœå­—å…¸
    """
    try:
        # åˆå¹¶åŸºå› è¡¨è¾¾å’Œç”Ÿå­˜æ•°æ®
        merged_df = pd.merge(
            pd.DataFrame({'expression': gene_data}),
            survival_df[['time', 'event', 'age']],
            left_index=True,
            right_index=True
        )
        
        if len(merged_df) < 20:  # æ ·æœ¬æ•°å¤ªå°‘
            return None
        
        # æ£€æŸ¥æ˜¯å¦æœ‰å¹´é¾„æ•°æ®
        has_age = not merged_df['age'].isna().all()
        
        # æ ¹æ®åˆ†ä½æ•°åˆ†ç»„ï¼ˆä½è¡¨è¾¾ç»„ï¼šæœ€ä½40%ï¼Œé«˜è¡¨è¾¾ç»„ï¼šæœ€é«˜40%ï¼‰
        low_quantile = merged_df['expression'].quantile(0.40)
        high_quantile = merged_df['expression'].quantile(0.60)
        
        high_group = merged_df[merged_df['expression'] >= high_quantile]
        low_group = merged_df[merged_df['expression'] <= low_quantile]
        
        if len(high_group) < 5 or len(low_group) < 5:
            return None
        
        # 1. Log-rankæ£€éªŒï¼ˆæœªè°ƒæ•´å¹´é¾„ï¼‰
        try:
            logrank_results = logrank_test(
                durations_A=high_group['time'], 
                event_observed_A=high_group['event'],
                durations_B=low_group['time'], 
                event_observed_B=low_group['event']
            )
            logrank_p = logrank_results.p_value
        except Exception:
            logrank_p = np.nan
        
        # 2. Coxå›å½’åˆ†æï¼ˆè°ƒæ•´å¹´é¾„ï¼‰
        cox_p_unadjusted = np.nan
        cox_p_adjusted = np.nan
        cox_hr_unadjusted = np.nan
        cox_hr_adjusted = np.nan
        cox_hr_ci_lower_unadjusted = np.nan
        cox_hr_ci_upper_unadjusted = np.nan
        cox_hr_ci_lower_adjusted = np.nan
        cox_hr_ci_upper_adjusted = np.nan
        
        try:
            # å‡†å¤‡Coxå›å½’æ•°æ®ï¼ˆè¿ç»­å˜é‡ï¼‰
            cox_df = merged_df.copy()
            cox_df = cox_df.dropna()
            
            if len(cox_df) >= 20:
                # å•å˜é‡Coxå›å½’ï¼ˆä»…åŸºå› è¡¨è¾¾ï¼‰
                try:
                    cph_univariate = CoxPHFitter()
                    cph_univariate.fit(cox_df[['time', 'event', 'expression']], 
                                     duration_col='time', event_col='event')
                    
                    cox_p_unadjusted = cph_univariate.summary.loc['expression', 'p']
                    cox_hr_unadjusted = cph_univariate.summary.loc['expression', 'exp(coef)']
                    cox_hr_ci_lower_unadjusted = cph_univariate.summary.loc['expression', 'exp(coef) lower 95%']
                    cox_hr_ci_upper_unadjusted = cph_univariate.summary.loc['expression', 'exp(coef) upper 95%']
                except Exception:
                    pass
                
                # å¤šå˜é‡Coxå›å½’ï¼ˆåŸºå› è¡¨è¾¾ + å¹´é¾„ï¼‰
                if has_age and not cox_df['age'].isna().all():
                    try:
                        cph_multivariate = CoxPHFitter()
                        cph_multivariate.fit(cox_df[['time', 'event', 'expression', 'age']], 
                                           duration_col='time', event_col='event')
                        
                        cox_p_adjusted = cph_multivariate.summary.loc['expression', 'p']
                        cox_hr_adjusted = cph_multivariate.summary.loc['expression', 'exp(coef)']
                        cox_hr_ci_lower_adjusted = cph_multivariate.summary.loc['expression', 'exp(coef) lower 95%']
                        cox_hr_ci_upper_adjusted = cph_multivariate.summary.loc['expression', 'exp(coef) upper 95%']
                    except Exception:
                        pass
        except Exception:
            pass
        
        # è®¡ç®—ä¸­ä½ç”Ÿå­˜æ—¶é—´
        kmf = KaplanMeierFitter()
        
        try:
            kmf.fit(high_group['time'], high_group['event'])
            median_survival_high = kmf.median_survival_time_
        except Exception:
            median_survival_high = np.nan
        
        try:
            kmf.fit(low_group['time'], low_group['event'])
            median_survival_low = kmf.median_survival_time_
        except Exception:
            median_survival_low = np.nan
        
        # å¹´é¾„ç»Ÿè®¡
        if has_age:
            age_mean_high = high_group['age'].mean()
            age_mean_low = low_group['age'].mean()
            age_p_value = np.nan
            
            # æ£€éªŒé«˜ä½è¡¨è¾¾ç»„å¹´é¾„å·®å¼‚
            try:
                from scipy.stats import ttest_ind
                _, age_p_value = ttest_ind(high_group['age'].dropna(), 
                                        low_group['age'].dropna())
            except Exception:
                pass
        else:
            age_mean_high = np.nan
            age_mean_low = np.nan
            age_p_value = np.nan
        
        return {
            'gene': gene_name,
            # Log-rankæ£€éªŒç»“æœ
            'logrank_p_value': logrank_p,
            'logrank_test_statistic': logrank_results.test_statistic if 'logrank_results' in locals() else np.nan,
            # Coxå›å½’ç»“æœï¼ˆæœªè°ƒæ•´ï¼‰
            'cox_p_unadjusted': cox_p_unadjusted,
            'cox_hr_unadjusted': cox_hr_unadjusted,
            'cox_hr_ci_lower_unadjusted': cox_hr_ci_lower_unadjusted,
            'cox_hr_ci_upper_unadjusted': cox_hr_ci_upper_unadjusted,
            # Coxå›å½’ç»“æœï¼ˆå¹´é¾„è°ƒæ•´ï¼‰
            'cox_p_adjusted': cox_p_adjusted,
            'cox_hr_adjusted': cox_hr_adjusted,
            'cox_hr_ci_lower_adjusted': cox_hr_ci_lower_adjusted,
            'cox_hr_ci_upper_adjusted': cox_hr_ci_upper_adjusted,
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            'high_group_n': len(high_group),
            'low_group_n': len(low_group),
            'high_group_events': high_group['event'].sum(),
            'low_group_events': low_group['event'].sum(),
            'median_survival_high': median_survival_high,
            'median_survival_low': median_survival_low,
            'expression_mean': merged_df['expression'].mean(),
            'expression_std': merged_df['expression'].std(),
            # å¹´é¾„ç›¸å…³ç»Ÿè®¡
            'age_mean_high': age_mean_high,
            'age_mean_low': age_mean_low,
            'age_difference_p_value': age_p_value,
            'has_age_data': has_age
        }
        
    except Exception as e:
        return None

def analyze_single_gene(gene_data, survival_df, gene_name):
    """
    å¯¹å•ä¸ªåŸºå› è¿›è¡Œç”Ÿå­˜åˆ†æï¼ˆä¿æŒå‘åå…¼å®¹æ€§ï¼‰
    """
    # å¦‚æœæœ‰å¹´é¾„æ•°æ®ï¼Œä½¿ç”¨è°ƒæ•´å¹´é¾„çš„åˆ†æ
    if 'age' in survival_df.columns and not survival_df['age'].isna().all():
        return analyze_single_gene_with_age_adjustment(gene_data, survival_df, gene_name)
    
    # å¦åˆ™ä½¿ç”¨åŸå§‹çš„åˆ†ææ–¹æ³•
    try:
        # åˆå¹¶åŸºå› è¡¨è¾¾å’Œç”Ÿå­˜æ•°æ®
        merged_df = pd.merge(
            pd.DataFrame({'expression': gene_data}),
            survival_df[['time', 'event']],
            left_index=True,
            right_index=True
        )
        
        if len(merged_df) < 20:  # æ ·æœ¬æ•°å¤ªå°‘
            return None
        
        # æ ¹æ®åˆ†ä½æ•°åˆ†ç»„ï¼ˆä½è¡¨è¾¾ç»„ï¼šæœ€ä½40%ï¼Œé«˜è¡¨è¾¾ç»„ï¼šæœ€é«˜40%ï¼‰
        low_quantile = merged_df['expression'].quantile(0.40)
        high_quantile = merged_df['expression'].quantile(0.60)
        
        high_group = merged_df[merged_df['expression'] >= high_quantile]
        low_group = merged_df[merged_df['expression'] <= low_quantile]
        
        if len(high_group) < 5 or len(low_group) < 5:
            return None
        
        # è¿›è¡ŒLog-rankæ£€éªŒ
        try:
            results = logrank_test(
                durations_A=high_group['time'], 
                event_observed_A=high_group['event'],
                durations_B=low_group['time'], 
                event_observed_B=low_group['event']
            )
            
            # è®¡ç®—ä¸­ä½ç”Ÿå­˜æ—¶é—´
            kmf = KaplanMeierFitter()
            
            kmf.fit(high_group['time'], high_group['event'])
            median_survival_high = kmf.median_survival_time_
            
            kmf.fit(low_group['time'], low_group['event'])
            median_survival_low = kmf.median_survival_time_
            
            return {
                'gene': gene_name,
                'p_value': results.p_value,
                'test_statistic': results.test_statistic,
                'high_group_n': len(high_group),
                'low_group_n': len(low_group),
                'high_group_events': high_group['event'].sum(),
                'low_group_events': low_group['event'].sum(),
                'median_survival_high': median_survival_high,
                'median_survival_low': median_survival_low,
                'expression_mean': merged_df['expression'].mean(),
                'expression_std': merged_df['expression'].std()
            }
            
        except Exception as e:
            return None
            
    except Exception as e:
        return None

def batch_survival_analysis(expr_df, survival_df, logger, n_cores=None):
    """
    æ‰¹é‡è¿›è¡Œç”Ÿå­˜åˆ†æ
    
    å‚æ•°ï¼š
        expr_df (DataFrame): åŸºå› è¡¨è¾¾æ•°æ®
        survival_df (DataFrame): ç”Ÿå­˜æ•°æ®
        logger: æ—¥å¿—è®°å½•å™¨
        n_cores (int): ä½¿ç”¨çš„CPUæ ¸å¿ƒæ•°
        
    è¿”å›ï¼š
        DataFrame: æ‰€æœ‰åŸºå› çš„ç”Ÿå­˜åˆ†æç»“æœ
    """
    logger.info("="*80)
    logger.info("å¼€å§‹æ‰¹é‡ç”Ÿå­˜åˆ†æ")
    logger.info("="*80)
    
    # è®¾ç½®å¹¶è¡Œå¤„ç† - å…è®¸ä½¿ç”¨CPUæ€»æ•°-1ä¸ªæ ¸å¿ƒ
    if n_cores is None:
        n_cores = mp.cpu_count() - 1  # ä½¿ç”¨CPUæ€»æ•°-1ä¸ªæ ¸å¿ƒ
    else:
        n_cores = min(n_cores, mp.cpu_count() - 1)  # é™åˆ¶ä¸è¶…è¿‡CPUæ€»æ•°-1
    
    logger.info(f"ç³»ç»Ÿå¯ç”¨CPUæ ¸å¿ƒæ•°: {mp.cpu_count()}")
    logger.info(f"å®é™…ä½¿ç”¨CPUæ ¸å¿ƒæ•°: {n_cores}")
    logger.info(f"æ€»å…±éœ€è¦åˆ†æåŸºå› æ•°: {len(expr_df)}")
    
    # ä¼°ç®—åˆ†ææ—¶é—´
    estimated_time_per_gene = 0.1  # æ¯ä¸ªåŸºå› å¤§çº¦0.1ç§’
    total_estimated_time = len(expr_df) * estimated_time_per_gene / n_cores
    logger.info(f"é¢„ä¼°æ€»åˆ†ææ—¶é—´: {total_estimated_time/60:.1f} åˆ†é’Ÿ")
    
    # å‡†å¤‡åˆ†æå‡½æ•°
    analyze_func = partial(analyze_single_gene, survival_df=survival_df)
    
    # å‡†å¤‡åŸºå› æ•°æ®
    logger.info("æ­£åœ¨å‡†å¤‡åŸºå› æ•°æ®...")
    gene_list = []
    failed_genes = []
    
    for i, gene_name in enumerate(expr_df.index):
        try:
            gene_data = expr_df.loc[gene_name]
            gene_data.name = gene_name
            
            # åŸºæœ¬æ•°æ®è´¨é‡æ£€æŸ¥
            if gene_data.isna().all():
                failed_genes.append((gene_name, "æ‰€æœ‰å€¼ä¸ºNaN"))
                continue
            if gene_data.std() == 0:
                failed_genes.append((gene_name, "è¡¨è¾¾å€¼ä¸ºå¸¸æ•°"))
                continue
                
            gene_list.append((gene_data, survival_df, gene_name))
            
        except Exception as e:
            failed_genes.append((gene_name, f"æ•°æ®å‡†å¤‡é”™è¯¯: {str(e)}"))
    
    logger.info(f"æ•°æ®å‡†å¤‡å®Œæˆ:")
    logger.info(f"  - å¯åˆ†æåŸºå› æ•°: {len(gene_list)}")
    logger.info(f"  - è·³è¿‡åŸºå› æ•°: {len(failed_genes)}")
    
    if failed_genes:
        logger.info(f"è·³è¿‡çš„åŸºå› ç¤ºä¾‹ (å‰5ä¸ª): {[f'{gene}({reason})' for gene, reason in failed_genes[:5]]}")
    
    if len(gene_list) == 0:
        logger.error("âœ— æ²¡æœ‰åŸºå› å¯ä»¥è¿›è¡Œåˆ†æ")
        return None
    
    # ä½¿ç”¨å¤šè¿›ç¨‹è¿›è¡Œåˆ†æ
    logger.info("="*50)
    logger.info("å¼€å§‹å¹¶è¡Œåˆ†æè¿›ç¨‹")
    logger.info("="*50)
    
    results = []
    successful_analyses = 0
    failed_analyses = 0
    
    # åˆ†æ‰¹å¤„ç†ä»¥æä¾›æ›´å¥½çš„è¿›åº¦åé¦ˆ
    batch_size = max(100, len(gene_list) // 20)  # åˆ†æˆå¤§çº¦20ä¸ªæ‰¹æ¬¡
    total_batches = (len(gene_list) + batch_size - 1) // batch_size
    
    logger.info(f"åˆ†æé…ç½®:")
    logger.info(f"  - æ‰¹æ¬¡å¤§å°: {batch_size} ä¸ªåŸºå› /æ‰¹æ¬¡")
    logger.info(f"  - æ€»æ‰¹æ¬¡æ•°: {total_batches}")
    logger.info(f"  - å¹¶è¡Œè¿›ç¨‹æ•°: {n_cores}")
    
    start_time = datetime.now()
    
    with mp.Pool(processes=n_cores) as pool:
        for batch_idx in range(total_batches):
            batch_start = batch_idx * batch_size
            batch_end = min((batch_idx + 1) * batch_size, len(gene_list))
            batch_genes = gene_list[batch_start:batch_end]
            
            logger.info(f"\næ‰¹æ¬¡ {batch_idx + 1}/{total_batches}: åˆ†æåŸºå›  {batch_start + 1}-{batch_end}")
            
            # åˆ†æå½“å‰æ‰¹æ¬¡
            batch_results = list(tqdm(
                pool.starmap(analyze_single_gene, batch_genes),
                total=len(batch_genes),
                desc=f"æ‰¹æ¬¡ {batch_idx + 1}/{total_batches}",
                ncols=100,
                leave=False
            ))
            
            # ç»Ÿè®¡å½“å‰æ‰¹æ¬¡ç»“æœ
            batch_successful = sum(1 for r in batch_results if r is not None)
            batch_failed = len(batch_results) - batch_successful
            successful_analyses += batch_successful
            failed_analyses += batch_failed
            
            results.extend(batch_results)
            
            # æ‰¹æ¬¡å®Œæˆç»Ÿè®¡
            elapsed_time = (datetime.now() - start_time).total_seconds()
            progress_pct = (batch_end / len(gene_list)) * 100
            estimated_remaining = elapsed_time / progress_pct * (100 - progress_pct) if progress_pct > 0 else 0
            
            logger.info(f"  âœ“ æ‰¹æ¬¡å®Œæˆ: {batch_successful}/{len(batch_genes)} ä¸ªåŸºå› æˆåŠŸåˆ†æ")
            logger.info(f"  æ€»è¿›åº¦: {progress_pct:.1f}% ({successful_analyses}/{len(gene_list)})")
            logger.info(f"  å·²ç”¨æ—¶é—´: {elapsed_time/60:.1f} åˆ†é’Ÿ")
            logger.info(f"  é¢„è®¡å‰©ä½™: {estimated_remaining/60:.1f} åˆ†é’Ÿ")
    
    total_time = (datetime.now() - start_time).total_seconds()
    
    logger.info("="*50)
    logger.info("å¹¶è¡Œåˆ†æå®Œæˆ")
    logger.info("="*50)
    logger.info(f"åˆ†æç»Ÿè®¡:")
    logger.info(f"  - æˆåŠŸåˆ†æ: {successful_analyses} ä¸ªåŸºå› ")
    logger.info(f"  - åˆ†æå¤±è´¥: {failed_analyses} ä¸ªåŸºå› ")
    logger.info(f"  - æˆåŠŸç‡: {successful_analyses/(successful_analyses+failed_analyses)*100:.1f}%")
    logger.info(f"  - æ€»è€—æ—¶: {total_time/60:.1f} åˆ†é’Ÿ")
    logger.info(f"  - å¹³å‡é€Ÿåº¦: {successful_analyses/total_time:.1f} åŸºå› /ç§’")
    
    # è¿‡æ»¤æ‰Noneç»“æœ
    valid_results = [r for r in results if r is not None]
    
    if len(valid_results) == 0:
        logger.error("âœ— æ²¡æœ‰åŸºå› é€šè¿‡ç”Ÿå­˜åˆ†æ")
        return None
    
    # è½¬æ¢ä¸ºDataFrame
    logger.info("æ­£åœ¨å¤„ç†åˆ†æç»“æœ...")
    results_df = pd.DataFrame(valid_results)
    
    # æ·»åŠ å¤šé‡æ£€éªŒæ ¡æ­£
    logger.info("æ­£åœ¨è¿›è¡Œå¤šé‡æ£€éªŒæ ¡æ­£...")
    from statsmodels.stats.multitest import fdrcorrection
    
    # æ£€æŸ¥ç»“æœæ ¼å¼ï¼Œå†³å®šä½¿ç”¨å“ªä¸ªPå€¼è¿›è¡Œæ ¡æ­£
    if 'cox_p_adjusted' in results_df.columns and not results_df['cox_p_adjusted'].isna().all():
        # ä½¿ç”¨å¹´é¾„è°ƒæ•´åçš„Coxå›å½’På€¼
        valid_p_values = results_df['cox_p_adjusted'].dropna()
        if len(valid_p_values) > 0:
            _, results_df['cox_p_adjusted_fdr'] = fdrcorrection(results_df['cox_p_adjusted'].fillna(1), alpha=0.05)
            primary_p_col = 'cox_p_adjusted'
            logger.info("ä½¿ç”¨å¹´é¾„è°ƒæ•´åçš„Coxå›å½’På€¼è¿›è¡Œå¤šé‡æ£€éªŒæ ¡æ­£")
        else:
            primary_p_col = 'logrank_p_value'
    elif 'cox_p_unadjusted' in results_df.columns and not results_df['cox_p_unadjusted'].isna().all():
        # ä½¿ç”¨æœªè°ƒæ•´çš„Coxå›å½’På€¼
        _, results_df['cox_p_unadjusted_fdr'] = fdrcorrection(results_df['cox_p_unadjusted'].fillna(1), alpha=0.05)
        primary_p_col = 'cox_p_unadjusted'
        logger.info("ä½¿ç”¨æœªè°ƒæ•´çš„Coxå›å½’På€¼è¿›è¡Œå¤šé‡æ£€éªŒæ ¡æ­£")
    elif 'logrank_p_value' in results_df.columns:
        # ä½¿ç”¨Log-rankæ£€éªŒPå€¼
        _, results_df['logrank_p_fdr'] = fdrcorrection(results_df['logrank_p_value'].fillna(1), alpha=0.05)
        primary_p_col = 'logrank_p_value'
        logger.info("ä½¿ç”¨Log-rankæ£€éªŒPå€¼è¿›è¡Œå¤šé‡æ£€éªŒæ ¡æ­£")
    else:
        # å…¼å®¹æ—§æ ¼å¼
        _, results_df['p_value_adj'] = fdrcorrection(results_df['p_value'].fillna(1), alpha=0.05)
        primary_p_col = 'p_value'
        logger.info("ä½¿ç”¨ä¼ ç»ŸPå€¼è¿›è¡Œå¤šé‡æ£€éªŒæ ¡æ­£")
    
    # æ’åº
    results_df = results_df.sort_values(primary_p_col).reset_index(drop=True)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    if primary_p_col == 'cox_p_adjusted':
        significant_genes = results_df[results_df['cox_p_adjusted'] < 0.05]
        highly_significant_genes = results_df[results_df['cox_p_adjusted'] < 0.01]
        adj_significant_genes = results_df[results_df['cox_p_adjusted_fdr'] < 0.05]
        min_p_value = results_df['cox_p_adjusted'].min()
        median_p_value = results_df['cox_p_adjusted'].median()
    elif primary_p_col == 'cox_p_unadjusted':
        significant_genes = results_df[results_df['cox_p_unadjusted'] < 0.05]
        highly_significant_genes = results_df[results_df['cox_p_unadjusted'] < 0.01]
        adj_significant_genes = results_df[results_df['cox_p_unadjusted_fdr'] < 0.05]
        min_p_value = results_df['cox_p_unadjusted'].min()
        median_p_value = results_df['cox_p_unadjusted'].median()
    elif primary_p_col == 'logrank_p_value':
        significant_genes = results_df[results_df['logrank_p_value'] < 0.05]
        highly_significant_genes = results_df[results_df['logrank_p_value'] < 0.01]
        adj_significant_genes = results_df[results_df['logrank_p_fdr'] < 0.05]
        min_p_value = results_df['logrank_p_value'].min()
        median_p_value = results_df['logrank_p_value'].median()
    else:
        # å…¼å®¹æ—§æ ¼å¼
        significant_genes = results_df[results_df['p_value'] < 0.05]
        highly_significant_genes = results_df[results_df['p_value'] < 0.01]
        adj_significant_genes = results_df[results_df['p_value_adj'] < 0.05]
        min_p_value = results_df['p_value'].min()
        median_p_value = results_df['p_value'].median()
    
    logger.info("="*80)
    logger.info("ç”Ÿå­˜åˆ†æç»Ÿè®¡ç»“æœ")
    logger.info("="*80)
    logger.info(f"ç»“æœæ¦‚è§ˆ:")
    logger.info(f"  - æ€»åˆ†æåŸºå› æ•°: {len(valid_results)}")
    logger.info(f"  - P < 0.05 çš„åŸºå› æ•°: {len(significant_genes)} ({len(significant_genes)/len(valid_results)*100:.1f}%)")
    logger.info(f"  - P < 0.01 çš„åŸºå› æ•°: {len(highly_significant_genes)} ({len(highly_significant_genes)/len(valid_results)*100:.1f}%)")
    logger.info(f"  - æ ¡æ­£åP < 0.05 çš„åŸºå› æ•°: {len(adj_significant_genes)} ({len(adj_significant_genes)/len(valid_results)*100:.1f}%)")
    logger.info(f"  - æœ€å°På€¼: {min_p_value:.2e}")
    logger.info(f"  - På€¼ä¸­ä½æ•°: {median_p_value:.3f}")
    
    # å¹´é¾„è°ƒæ•´æ•ˆæœåˆ†æ
    if 'cox_p_adjusted' in results_df.columns and 'cox_p_unadjusted' in results_df.columns:
        # æ¯”è¾ƒè°ƒæ•´å‰åçš„æ˜¾è‘—åŸºå› æ•°
        unadj_sig = len(results_df[results_df['cox_p_unadjusted'] < 0.05])
        adj_sig = len(results_df[results_df['cox_p_adjusted'] < 0.05])
        
        logger.info(f"\nå¹´é¾„è°ƒæ•´æ•ˆæœ:")
        logger.info(f"  - æœªè°ƒæ•´å¹´é¾„æ—¶æ˜¾è‘—åŸºå› æ•°: {unadj_sig}")
        logger.info(f"  - è°ƒæ•´å¹´é¾„åæ˜¾è‘—åŸºå› æ•°: {adj_sig}")
        logger.info(f"  - å˜åŒ–: {adj_sig - unadj_sig:+d} ä¸ªåŸºå› ")
        
        if adj_sig < unadj_sig:
            logger.info(f"  ğŸ’¡ å¹´é¾„è°ƒæ•´å‡å°‘äº† {unadj_sig - adj_sig} ä¸ªå‡é˜³æ€§ç»“æœ")
        elif adj_sig > unadj_sig:
            logger.info(f"  ğŸ’¡ å¹´é¾„è°ƒæ•´å‘ç°äº† {adj_sig - unadj_sig} ä¸ªæ–°çš„æ˜¾è‘—åŸºå› ")
        else:
            logger.info(f"  ğŸ’¡ å¹´é¾„è°ƒæ•´å¯¹ç»“æœå½±å“è¾ƒå°")
    
    # æ˜¾ç¤ºå‰10ä¸ªæœ€æ˜¾è‘—çš„åŸºå› 
    logger.info(f"\nå‰10ä¸ªæœ€æ˜¾è‘—çš„åŸºå› :")
    for i, (_, gene_info) in enumerate(results_df.head(10).iterrows()):
        if primary_p_col == 'cox_p_adjusted':
            p_val = gene_info['cox_p_adjusted']
            hr_info = f"HR={gene_info.get('cox_hr_adjusted', 'NA'):.3f}" if pd.notna(gene_info.get('cox_hr_adjusted')) else ""
        elif primary_p_col == 'cox_p_unadjusted':
            p_val = gene_info['cox_p_unadjusted']
            hr_info = f"HR={gene_info.get('cox_hr_unadjusted', 'NA'):.3f}" if pd.notna(gene_info.get('cox_hr_unadjusted')) else ""
        elif primary_p_col == 'logrank_p_value':
            p_val = gene_info['logrank_p_value']
            hr_info = ""
        else:
            p_val = gene_info['p_value']
            hr_info = ""
        
        logger.info(f"  {i+1:2d}. {gene_info['gene']:20s} P={p_val:.2e} {hr_info} "
                   f"(é«˜è¡¨è¾¾ç»„n={gene_info['high_group_n']}, ä½è¡¨è¾¾ç»„n={gene_info['low_group_n']})")
    
    return results_df

def plot_km_curves_for_top_genes(expr_df, survival_df, results_df, output_dir, top_n=10, logger=None):
    """
    ä¸ºæœ€æ˜¾è‘—çš„åŸºå› ç»˜åˆ¶KMæ›²çº¿
    
    ã€åˆ†ç»„ç­–ç•¥è¯¦ç»†è¯´æ˜ã€‘
    ===================
    
    1. åŸºå› è¡¨è¾¾åˆ†ç»„æ–¹æ³•ï¼š
       - è®¡ç®—æ¯ä¸ªåŸºå› åœ¨æ‰€æœ‰æ ·æœ¬ä¸­çš„è¡¨è¾¾å€¼åˆ†å¸ƒ
       - ä½¿ç”¨åˆ†ä½æ•°è¿›è¡Œåˆ†ç»„ï¼š
         * é«˜è¡¨è¾¾ç»„ï¼šè¡¨è¾¾å€¼ â‰¥ 60%åˆ†ä½æ•° (æœ€é«˜40%)
         * ä½è¡¨è¾¾ç»„ï¼šè¡¨è¾¾å€¼ â‰¤ 40%åˆ†ä½æ•° (æœ€ä½40%)
         * ä¸­ç­‰è¡¨è¾¾ç»„ï¼š40%åˆ†ä½æ•° < è¡¨è¾¾å€¼ < 60%åˆ†ä½æ•° (ä¸­é—´20%ï¼Œæ’é™¤ä¸å‚ä¸æ¯”è¾ƒ)
    
    2. åˆ†ç»„é€»è¾‘åŸç†ï¼š
       - æ’é™¤ä¸­ç­‰è¡¨è¾¾ç»„æ˜¯ä¸ºäº†å¢å¼ºç»„é—´å¯¹æ¯”æ•ˆæœ
       - è¿™ç§åˆ†ç»„æ–¹æ³•ç¡®ä¿äº†é«˜ä½è¡¨è¾¾ç»„æœ‰æ˜æ˜¾çš„è¡¨è¾¾å·®å¼‚
       - ä½¿ç”¨40%/60%åˆ†ä½æ•°æ¯”ä¼ ç»Ÿçš„25%/75%åˆ†ä½æ•°åŒ…å«æ›´å¤šæ ·æœ¬ï¼Œæé«˜ç»Ÿè®¡æ£€éªŒåŠŸæ•ˆ
       - é¿å…äº†ä»»æ„äºŒåˆ†æ³•å¯èƒ½é€ æˆçš„ç»Ÿè®¡åå€š
       
    3. æ ·æœ¬æ•°è¦æ±‚ï¼š
       - æ¯ç»„è‡³å°‘éœ€è¦5ä¸ªæ ·æœ¬æ‰èƒ½è¿›è¡Œå¯é çš„ç”Ÿå­˜åˆ†æ
       - æ€»æ ·æœ¬æ•°è‡³å°‘20ä¸ªæ‰è¿›è¡Œåˆ†æ
       
    4. ç»Ÿè®¡æ£€éªŒï¼š
       - Log-rankæ£€éªŒï¼šæ¯”è¾ƒä¸¤ç»„ç”Ÿå­˜æ›²çº¿æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚
       - å¦‚æœæœ‰å¹´é¾„æ•°æ®ï¼Œä¼šé¢å¤–è¿›è¡ŒCoxå›å½’åˆ†æè°ƒæ•´å¹´é¾„å› ç´ 
       
    5. ç»“æœè§£é‡Šï¼š
       - På€¼ < 0.05ï¼šè¡¨ç¤ºè¯¥åŸºå› çš„é«˜ä½è¡¨è¾¾å¯¹æ‚£è€…ç”Ÿå­˜æœ‰æ˜¾è‘—å½±å“
       - é£é™©æ¯”(HR)ï¼š>1è¡¨ç¤ºé«˜è¡¨è¾¾å¢åŠ æ­»äº¡é£é™©ï¼Œ<1è¡¨ç¤ºé«˜è¡¨è¾¾æœ‰ä¿æŠ¤ä½œç”¨
    
    å‚æ•°ï¼š
        expr_df (DataFrame): åŸºå› è¡¨è¾¾æ•°æ®
        survival_df (DataFrame): ç”Ÿå­˜æ•°æ®
        results_df (DataFrame): ç”Ÿå­˜åˆ†æç»“æœ
        output_dir (Path): è¾“å‡ºç›®å½•
        top_n (int): ç»˜åˆ¶å‰Nä¸ªæœ€æ˜¾è‘—çš„åŸºå› 
        logger: æ—¥å¿—è®°å½•å™¨
    """
    if logger:
        logger.info("="*80)
        logger.info(f"å¼€å§‹ä¸ºå‰{top_n}ä¸ªæœ€æ˜¾è‘—åŸºå› ç»˜åˆ¶KMæ›²çº¿")
        logger.info("="*80)
    
    # é€‰æ‹©å‰Nä¸ªæœ€æ˜¾è‘—çš„åŸºå› 
    top_genes = results_df.head(top_n)
    
    # è®¾ç½®å›¾å½¢æ ·å¼
    plt.style.use('default')
    sns.set_palette("husl")
    
    for idx, (_, gene_info) in enumerate(top_genes.iterrows()):
        gene_name = gene_info['gene']
        
        # ç¡®å®šä½¿ç”¨å“ªä¸ªPå€¼
        if 'cox_p_adjusted' in gene_info and pd.notna(gene_info['cox_p_adjusted']):
            p_value = gene_info['cox_p_adjusted']
            p_label = 'Cox P-value (Age Adjusted)'
        elif 'cox_p_unadjusted' in gene_info and pd.notna(gene_info['cox_p_unadjusted']):
            p_value = gene_info['cox_p_unadjusted']
            p_label = 'Cox P-value'
        elif 'logrank_p_value' in gene_info and pd.notna(gene_info['logrank_p_value']):
            p_value = gene_info['logrank_p_value']
            p_label = 'Log-rank P-value'
        else:
            p_value = gene_info.get('p_value', np.nan)
            p_label = 'P-value'
        
        if logger:
            logger.info(f"æ­£åœ¨ç»˜åˆ¶åŸºå›  {gene_name} çš„KMæ›²çº¿ ({p_label}={p_value:.2e})")
        
        # è·å–åŸºå› è¡¨è¾¾æ•°æ®
        gene_data = expr_df.loc[gene_name]
        
        # åˆå¹¶æ•°æ®
        merged_df = pd.merge(
            pd.DataFrame({'expression': gene_data}),
            survival_df[['time', 'event']],
            left_index=True,
            right_index=True
        )
        
        # åˆ†ç»„ï¼ˆä½è¡¨è¾¾ç»„ï¼šæœ€ä½40%ï¼Œé«˜è¡¨è¾¾ç»„ï¼šæœ€é«˜40%ï¼‰
        low_quantile = merged_df['expression'].quantile(0.40)
        high_quantile = merged_df['expression'].quantile(0.60)
        
        high_group = merged_df[merged_df['expression'] >= high_quantile]
        low_group = merged_df[merged_df['expression'] <= low_quantile]
        
        # ç»˜åˆ¶KMæ›²çº¿
        plt.figure(figsize=(10, 8))
        
        kmf = KaplanMeierFitter()
        
        # é«˜è¡¨è¾¾ç»„
        kmf.fit(
            durations=high_group['time'], 
            event_observed=high_group['event'], 
            label=f'High {gene_name} (n={len(high_group)})'
        )
        ax = kmf.plot(ci_show=False, linewidth=2.5)
        
        # ä½è¡¨è¾¾ç»„
        kmf.fit(
            durations=low_group['time'], 
            event_observed=low_group['event'], 
            label=f'Low {gene_name} (n={len(low_group)})'
        )
        kmf.plot(ax=ax, ci_show=False, linewidth=2.5)
        
        # ç¾åŒ–å›¾è¡¨
        plt.title(f'Kaplan-Meier Survival Curve - {gene_name}', 
                 fontsize=16, fontweight='bold', pad=20)
        plt.xlabel('Time (days)', fontsize=14, fontweight='bold')
        plt.ylabel('Survival Probability', fontsize=14, fontweight='bold')
        plt.grid(True, linestyle='--', alpha=0.3)
        
        # æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
        plt.text(0.02, 0.02, f'{p_label}: {p_value:.2e}', 
                transform=ax.transAxes, fontsize=12, fontweight='bold',
                bbox=dict(boxstyle='round,pad=0.5', facecolor='white', alpha=0.8))
        
        # æ·»åŠ æ’åä¿¡æ¯
        plt.text(0.98, 0.98, f'Rank: #{idx+1}', 
                transform=ax.transAxes, fontsize=12, fontweight='bold',
                ha='right', va='top',
                bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))
        
        plt.legend(fontsize=12, frameon=True, fancybox=True, shadow=True)
        plt.tight_layout()
        
        # ä¿å­˜å›¾è¡¨
        safe_gene_name = sanitize_filename(gene_name)
        output_path = output_dir / f"KM_curve_{idx+1:02d}_{safe_gene_name}"
        plt.savefig(f"{output_path}.png", dpi=300, bbox_inches='tight')
        plt.savefig(f"{output_path}.pdf", bbox_inches='tight')
        plt.close()
    
    if logger:
        logger.info(f"âœ“ å®Œæˆæ‰€æœ‰KMæ›²çº¿ç»˜åˆ¶")

def create_summary_plots(results_df, output_dir, logger):
    """
    åˆ›å»ºæ±‡æ€»å›¾è¡¨
    
    å‚æ•°ï¼š
        results_df (DataFrame): ç”Ÿå­˜åˆ†æç»“æœ
        output_dir (Path): è¾“å‡ºç›®å½•
        logger: æ—¥å¿—è®°å½•å™¨
    """
    logger.info("æ­£åœ¨åˆ›å»ºæ±‡æ€»å›¾è¡¨...")
    
    # ç¡®å®šä½¿ç”¨å“ªä¸ªPå€¼åˆ—è¿›è¡Œç»˜å›¾
    if 'cox_p_adjusted' in results_df.columns and not results_df['cox_p_adjusted'].isna().all():
        p_col = 'cox_p_adjusted'
        p_title = 'Cox P-value (Age Adjusted)'
    elif 'cox_p_unadjusted' in results_df.columns and not results_df['cox_p_unadjusted'].isna().all():
        p_col = 'cox_p_unadjusted'
        p_title = 'Cox P-value (Unadjusted)'
    elif 'logrank_p_value' in results_df.columns:
        p_col = 'logrank_p_value'
        p_title = 'Log-rank P-value'
    else:
        p_col = 'p_value'  # å…¼å®¹æ—§æ ¼å¼
        p_title = 'P-value'
    
    # 1. På€¼åˆ†å¸ƒç›´æ–¹å›¾
    plt.figure(figsize=(12, 8))
    
    plt.subplot(2, 2, 1)
    p_values = results_df[p_col].dropna()
    plt.hist(p_values, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    plt.axvline(x=0.05, color='red', linestyle='--', label='P=0.05')
    plt.axvline(x=0.01, color='orange', linestyle='--', label='P=0.01')
    plt.xlabel(p_title)
    plt.ylabel('åŸºå› æ•°é‡')
    plt.title(f'{p_title}åˆ†å¸ƒç›´æ–¹å›¾')
    plt.legend()
    
    # 2. -log10(På€¼) vs åŸºå› æ’å
    plt.subplot(2, 2, 2)
    ranks = range(1, len(p_values) + 1)
    plt.plot(ranks, -np.log10(p_values), 'o-', markersize=3, alpha=0.7)
    plt.axhline(y=-np.log10(0.05), color='red', linestyle='--', label='P=0.05')
    plt.axhline(y=-np.log10(0.01), color='orange', linestyle='--', label='P=0.01')
    plt.xlabel('åŸºå› æ’å')
    plt.ylabel(f'-log10({p_title})')
    plt.title('æ˜¾è‘—æ€§æ’åå›¾')
    plt.legend()
    
    # 3. æ ·æœ¬æ•°åˆ†å¸ƒ
    plt.subplot(2, 2, 3)
    total_samples = results_df['high_group_n'] + results_df['low_group_n']
    plt.hist(total_samples, bins=30, alpha=0.7, color='lightgreen', edgecolor='black')
    plt.xlabel('åˆ†ææ ·æœ¬æ•°')
    plt.ylabel('åŸºå› æ•°é‡')
    plt.title('æ ·æœ¬æ•°åˆ†å¸ƒ')
    
    # 4. å‰20ä¸ªæœ€æ˜¾è‘—åŸºå› çš„æ¡å½¢å›¾
    plt.subplot(2, 2, 4)
    top_20 = results_df.head(20)
    y_pos = range(len(top_20))
    plt.barh(y_pos, -np.log10(top_20[p_col]), color='coral', alpha=0.8)
    plt.yticks(y_pos, [gene[:15] + '...' if len(gene) > 15 else gene for gene in top_20['gene']])
    plt.xlabel(f'-log10({p_title})')
    plt.title('å‰20ä¸ªæœ€æ˜¾è‘—åŸºå› ')
    plt.gca().invert_yaxis()
    
    plt.tight_layout()
    
    # ä¿å­˜æ±‡æ€»å›¾
    summary_path = output_dir / "survival_analysis_summary"
    plt.savefig(f"{summary_path}.png", dpi=300, bbox_inches='tight')
    plt.savefig(f"{summary_path}.pdf", bbox_inches='tight')
    plt.close()
    
    logger.info(f"âœ“ æ±‡æ€»å›¾è¡¨å·²ä¿å­˜åˆ° {summary_path.name}")

def main():
    """
    æ‰¹é‡ç”Ÿå­˜åˆ†æä¸»å‡½æ•°
    """
    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="æ‰¹é‡ç”Ÿå­˜åˆ†æ - è¯†åˆ«ä¸æ‚£è€…ç”Ÿå­˜é¢„åæœ€ç›¸å…³çš„åŸºå› ")
    parser.add_argument(
        "--expression_file",
        type=Path,
        default=PROJECT_ROOT / "data" / "processed" / "working.csv",
        help="åŸºå› è¡¨è¾¾æ•°æ®æ–‡ä»¶è·¯å¾„ (CSVæ ¼å¼)"
    )
    parser.add_argument(
        "--clinical_file", 
        type=Path,
        default=PROJECT_ROOT / "data" / "processed" / "OV.clin.merged.picked.txt",
        help="ä¸´åºŠæ•°æ®æ–‡ä»¶è·¯å¾„"
    )
    parser.add_argument(
        "--output_dir",
        type=Path,
        default=PROJECT_ROOT / "results" / "batch_survival_analysis",
        help="è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--top_n",
        type=int,
        default=20,
        help="ç»˜åˆ¶å‰Nä¸ªæœ€æ˜¾è‘—åŸºå› çš„KMæ›²çº¿ (é»˜è®¤: 20)"
    )
    parser.add_argument(
        "--n_cores",
        type=int,
        default=None,
        help="ä½¿ç”¨çš„CPUæ ¸å¿ƒæ•° (é»˜è®¤: CPUæ€»æ•°-1)"
    )
    parser.add_argument(
        "--skip_preview",
        action="store_true",
        help="è·³è¿‡æ•°æ®é¢„è§ˆç¡®è®¤æ­¥éª¤ï¼Œç›´æ¥å¼€å§‹åˆ†æ"
    )
    parser.add_argument(
        "--adjust_age",
        action="store_true",
        default=True,
        help="æ˜¯å¦è°ƒæ•´å¹´é¾„æ··æ‚å› ç´  (é»˜è®¤: True)"
    )
    parser.add_argument(
        "--no_adjust_age",
        action="store_true",
        help="ä¸è°ƒæ•´å¹´é¾„æ··æ‚å› ç´ ï¼ˆä½¿ç”¨ä¼ ç»ŸLog-rankæ£€éªŒï¼‰"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    args.output_dir.mkdir(parents=True, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—
    logger = setup_logging(args.output_dir)
    
    # è®°å½•å¼€å§‹æ—¶é—´
    start_time = datetime.now()
    logger.info("="*80)
    logger.info("æ‰¹é‡ç”Ÿå­˜åˆ†æå¼€å§‹")
    logger.info("="*80)
    logger.info(f"åˆ†æå¼€å§‹æ—¶é—´: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"é…ç½®å‚æ•°:")
    logger.info(f"  - è¡¨è¾¾æ•°æ®æ–‡ä»¶: {args.expression_file.relative_to(PROJECT_ROOT)}")
    logger.info(f"  - ä¸´åºŠæ•°æ®æ–‡ä»¶: {args.clinical_file.relative_to(PROJECT_ROOT)}")
    logger.info(f"  - è¾“å‡ºç›®å½•: {args.output_dir.relative_to(PROJECT_ROOT)}")
    logger.info(f"  - ç»˜åˆ¶KMæ›²çº¿æ•°: {args.top_n}")
    logger.info(f"  - CPUæ ¸å¿ƒæ•°: {args.n_cores}")
    logger.info(f"  - è·³è¿‡æ•°æ®é¢„è§ˆ: {'æ˜¯' if args.skip_preview else 'å¦'}")
    logger.info(f"  - è°ƒæ•´å¹´é¾„: {'æ˜¯' if args.adjust_age else 'å¦'}")
    
    try:
        # 1. å‡†å¤‡æ•°æ®
        expr_df, survival_df = prepare_survival_data(args.expression_file, args.clinical_file, logger)
        if expr_df is None or survival_df is None:
            logger.error("âœ— æ•°æ®å‡†å¤‡å¤±è´¥ï¼Œç»ˆæ­¢åˆ†æ")
            return
        
        # 2. å¦‚æœæœªè·³è¿‡é¢„è§ˆï¼Œç­‰å¾…ç”¨æˆ·ç¡®è®¤
        if not args.skip_preview:
            logger.info("="*80)
            logger.info("æ•°æ®é¢„è§ˆæ–‡ä»¶å·²ä¿å­˜ï¼Œè¯·æ£€æŸ¥ä»¥ä¸‹æ–‡ä»¶:")
            logger.info("="*80)
            logger.info("1. survival_data_preview.csv - ç”Ÿå­˜æ•°æ®é¢„è§ˆ")
            logger.info("2. expression_data_preview_top10_genes.csv - è¡¨è¾¾æ•°æ®é¢„è§ˆ")
            logger.info("3. data_summary_statistics.csv - æ•°æ®ç»Ÿè®¡ä¿¡æ¯")
            logger.info("")
            logger.info("è¯·æ£€æŸ¥è¿™äº›æ–‡ä»¶ä»¥ç¡®è®¤æ•°æ®è´¨é‡ã€‚")
            logger.info("å¦‚æœæ•°æ®çœ‹èµ·æ¥æ­£å¸¸ï¼Œè¯·æŒ‰å›è½¦ç»§ç»­åˆ†æ...")
            logger.info("å¦‚æœéœ€è¦åœæ­¢åˆ†æï¼Œè¯·æŒ‰ Ctrl+C")
            
            try:
                input("æŒ‰å›è½¦é”®ç»§ç»­åˆ†æï¼Œæˆ–æŒ‰ Ctrl+C åœæ­¢...")
                logger.info("âœ“ ç”¨æˆ·ç¡®è®¤ç»§ç»­åˆ†æ")
            except KeyboardInterrupt:
                logger.info("âœ— ç”¨æˆ·é€‰æ‹©åœæ­¢åˆ†æ")
                return
        else:
            logger.info("è·³è¿‡æ•°æ®é¢„è§ˆç¡®è®¤æ­¥éª¤ï¼Œç›´æ¥å¼€å§‹åˆ†æ")
        
        # 3. æ‰¹é‡ç”Ÿå­˜åˆ†æ
        analysis_start_time = datetime.now()
        logger.info("="*80)
        logger.info(f"æ­£å¼å¼€å§‹æ‰¹é‡ç”Ÿå­˜åˆ†æ: {analysis_start_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info("="*80)
        
        results_df = batch_survival_analysis(expr_df, survival_df, logger, args.n_cores)
        if results_df is None:
            logger.error("âœ— æ‰¹é‡ç”Ÿå­˜åˆ†æå¤±è´¥")
            return
        
        analysis_end_time = datetime.now()
        analysis_duration = (analysis_end_time - analysis_start_time).total_seconds()
        logger.info(f"âœ“ åˆ†æå®Œæˆï¼Œè€—æ—¶: {analysis_duration/60:.1f} åˆ†é’Ÿ")
        
        # 4. ä¿å­˜ç»“æœ
        logger.info("="*80)
        logger.info("ä¿å­˜åˆ†æç»“æœ")
        logger.info("="*80)
        
        results_file = args.output_dir / "all_genes_survival_analysis_results.csv"
        results_df.to_csv(results_file, index=False)
        logger.info(f"âœ“ å®Œæ•´ç»“æœå·²ä¿å­˜åˆ°: {results_file.name}")
        
        # 5. ä¿å­˜å‰100ä¸ªæœ€æ˜¾è‘—åŸºå› çš„è¯¦ç»†ç»“æœ
        top_100_file = args.output_dir / "top_100_significant_genes.csv"
        results_df.head(100).to_csv(top_100_file, index=False)
        logger.info(f"âœ“ å‰100ä¸ªæ˜¾è‘—åŸºå› å·²ä¿å­˜åˆ°: {top_100_file.name}")
        
        # 6. ä¿å­˜æ˜¾è‘—åŸºå› åˆ—è¡¨
        # ç¡®å®šä½¿ç”¨å“ªä¸ªPå€¼åˆ—
        if 'cox_p_adjusted' in results_df.columns and not results_df['cox_p_adjusted'].isna().all():
            p_col = 'cox_p_adjusted'
        elif 'cox_p_unadjusted' in results_df.columns and not results_df['cox_p_unadjusted'].isna().all():
            p_col = 'cox_p_unadjusted'
        elif 'logrank_p_value' in results_df.columns:
            p_col = 'logrank_p_value'
        else:
            p_col = 'p_value'  # å…¼å®¹æ—§æ ¼å¼
        
        significant_genes = results_df[results_df[p_col] < 0.05]
        if len(significant_genes) > 0:
            sig_file = args.output_dir / "significant_genes_p005.csv"
            significant_genes.to_csv(sig_file, index=False)
            logger.info(f"âœ“ æ˜¾è‘—åŸºå› (P<0.05)å·²ä¿å­˜åˆ°: {sig_file.name} (ä½¿ç”¨{p_col})")
        
        # 7. ç»˜åˆ¶KMæ›²çº¿
        logger.info("="*80)
        logger.info("ç”Ÿæˆå¯è§†åŒ–ç»“æœ")
        logger.info("="*80)
        
        plot_km_curves_for_top_genes(
            expr_df, survival_df, results_df, 
            args.output_dir, args.top_n, logger
        )
        
        # 8. åˆ›å»ºæ±‡æ€»å›¾è¡¨
        create_summary_plots(results_df, args.output_dir, logger)
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = datetime.now()
        total_duration = (end_time - start_time).total_seconds()
        
        logger.info("="*80)
        logger.info("æ‰¹é‡ç”Ÿå­˜åˆ†æå®Œæˆ")
        logger.info("="*80)
        logger.info(f"åˆ†æç»“æŸæ—¶é—´: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
        logger.info(f"æ€»è€—æ—¶: {total_duration/60:.1f} åˆ†é’Ÿ")
        logger.info(f"å¹³å‡æ¯ä¸ªåŸºå› è€—æ—¶: {analysis_duration/len(results_df):.3f} ç§’")
        
        # è¾“å‡ºæœ€ç»ˆç»“æœæ€»ç»“
        logger.info(f"\næœ€ç»ˆç»“æœæ€»ç»“:")
        logger.info(f"  - æˆåŠŸåˆ†æåŸºå› æ•°: {len(results_df)}")
        logger.info(f"  - æ˜¾è‘—åŸºå› æ•°(P<0.05): {len(significant_genes)}")
        
        # è®¡ç®—æ ¡æ­£åæ˜¾è‘—åŸºå› æ•°
        if 'cox_p_adjusted_fdr' in results_df.columns:
            adj_sig_count = len(results_df[results_df['cox_p_adjusted_fdr'] < 0.05])
        elif 'cox_p_unadjusted_fdr' in results_df.columns:
            adj_sig_count = len(results_df[results_df['cox_p_unadjusted_fdr'] < 0.05])
        elif 'logrank_p_fdr' in results_df.columns:
            adj_sig_count = len(results_df[results_df['logrank_p_fdr'] < 0.05])
        elif 'p_value_adj' in results_df.columns:
            adj_sig_count = len(results_df[results_df['p_value_adj'] < 0.05])
        else:
            adj_sig_count = 0
        
        logger.info(f"  - æ ¡æ­£åæ˜¾è‘—åŸºå› æ•°: {adj_sig_count}")
        logger.info(f"  - æœ€å°På€¼: {results_df[p_col].min():.2e}")
        
        # è¾“å‡ºå‰10ä¸ªæœ€æ˜¾è‘—åŸºå› 
        logger.info(f"\nå‰10ä¸ªæœ€æ˜¾è‘—çš„é¢„ååŸºå› :")
        for i, (_, gene_info) in enumerate(results_df.head(10).iterrows()):
            p_value = gene_info[p_col]
            logger.info(f"  {i+1:2d}. {gene_info['gene']:15s} "
                       f"P={p_value:.2e} "
                       f"(é«˜è¡¨è¾¾ç»„: {gene_info['high_group_n']}, "
                       f"ä½è¡¨è¾¾ç»„: {gene_info['low_group_n']})")
        
        logger.info(f"\nåˆ†ææˆåŠŸå®Œæˆï¼æ‰€æœ‰ç»“æœå·²ä¿å­˜åˆ°: {args.output_dir.relative_to(PROJECT_ROOT)}")
        
    except Exception as e:
        logger.error(f"âœ— åˆ†æè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        logger.error(f"é”™è¯¯è¯¦æƒ…: {traceback.format_exc()}")
        return

if __name__ == "__main__":
    main() 